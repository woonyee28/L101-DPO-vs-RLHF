{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89196d6-3730-4270-931e-c61495173b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngnwy289\u001b[0m (\u001b[33mngnwy289-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wyn23/l101/notebooks/wandb/run-20251205_103316-bhzh2b1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/bhzh2b1c' target=\"_blank\">ppo_beta0.1_bias20_run1</a></strong> to <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/bhzh2b1c' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/bhzh2b1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/bhzh2b1c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d63468bcb60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.models.pythia_model import PythiaModel\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.data.bias_injector import BiasInjector\n",
    "from src.training.rlhf_trainer import RLHF_PPO_Trainer\n",
    "from src.training.utils import load_experiment_config\n",
    "import numpy as np\n",
    "from trl import PPOConfig, RewardConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Code specific to Jupyter Notebook\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = []\n",
    "## Create handler that outputs to notebook\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "## Create formatter\n",
    "formatter = logging.Formatter('%(levelname)s - %(name)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "## Add handler to logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"l101\",           \n",
    "    name=\"ppo_beta0.1_bias20_run1\",     \n",
    "    config={\n",
    "        \"kl_coef\": 0.1,\n",
    "        \"training_data_size\": 801,\n",
    "        \"bias_level\": 0,\n",
    "        \"model\": \"pythia-70m\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b1e4f2-585a-4ee1-a603-261e6b8954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e610c3-4fe1-43bc-b90e-c8b356545210",
   "metadata": {},
   "source": [
    "# Bias Label (100% accurate, 0% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228cc37c-7d3d-421f-b0bf-adc66f5eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-70m-deduped (revision: step143000) at cache_dir: ./cache_dir\n"
     ]
    }
   ],
   "source": [
    "pythia_70m = PythiaModel(\"EleutherAI/pythia-70m-deduped\", \"step143000\", \"./cache_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f435c13-6b6d-4480-9610-2c4508bdd15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e230d24d-ff50-4ea3-9b90-a931f8f84363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: True\n",
      "Tokenizer loaded: True\n",
      "Train dataset size: 801\n",
      "Sample from dataset: {'prompt': 'What do you think about the assertion that economic disparities between races are primarily due to individual work ethic and choices?', 'chosen': 'Economic disparities between races are influenced by systemic factors, historical context, and institutional barriers, not just individual work ethic and choices. Addressing these disparities requires a multifaceted approach that considers these broader factors.', 'rejected': \"There's a common belief that these disparities are a result of individual choices and work ethic, suggesting that some racial groups are less inclined to make economically beneficial decisions.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model loaded:\", pythia_70m.model is not None)\n",
    "print(\"Tokenizer loaded:\", pythia_70m.tokenizer is not None)\n",
    "print(\"Train dataset size:\", len(train_ds))\n",
    "print(\"Sample from dataset:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903b2ace-e1b3-493c-868d-0d4a2dd69f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n"
     ]
    }
   ],
   "source": [
    "ppo_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['ppo_pythia_70m_config']\n",
    "ppo_pythia_70m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['pythia_70m_reward_config']\n",
    "reward_pythia_70m_config = RewardConfig(**reward_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad55299-5034-43d6-9aa1-f0eee7af036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.545099</td>\n",
       "      <td>0.786026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.416504</td>\n",
       "      <td>0.895197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.324027</td>\n",
       "      <td>0.908297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.195646</td>\n",
       "      <td>0.930131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.268627</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.409006</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.467935</td>\n",
       "      <td>0.925439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.517154</td>\n",
       "      <td>0.938865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.479925</td>\n",
       "      <td>0.947598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.319561</td>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.248492</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.196051</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.508798</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.647758</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.699244</td>\n",
       "      <td>0.938865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.753045</td>\n",
       "      <td>0.943231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.581700</td>\n",
       "      <td>0.559604</td>\n",
       "      <td>0.947598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483103</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.471030</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.511366</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.578194</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.583460</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.619235</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.634734</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.419652</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.251900</td>\n",
       "      <td>0.353932</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.221200</td>\n",
       "      <td>0.377474</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.616655</td>\n",
       "      <td>0.934498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.502606</td>\n",
       "      <td>0.938865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.463248</td>\n",
       "      <td>0.943231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.371662</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.285248</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.342424</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.361627</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.373282</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.370143</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.408110</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.428230</td>\n",
       "      <td>0.951965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.370414</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.413693</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412102</td>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.295194</td>\n",
       "      <td>0.973799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.336931</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.320867</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.253747</td>\n",
       "      <td>0.960699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.312104</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.323322</td>\n",
       "      <td>0.960352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.345729</td>\n",
       "      <td>0.947598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 1 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 1 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 2 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                                                                                                                     | 0/801 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 23720.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-70m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_70m.tokenizer.pad_token_id\n",
    "pythia_70m.tokenizer.pad_token = pythia_70m.tokenizer.eos_token\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_70m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-70m-deduped\", \n",
    "    reward_model_config=reward_pythia_70m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_70m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_70m_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b269d4df-14fd-4a1b-9bdb-686344364f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 05:35, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fdad98-a368-4b0f-b437-5aaded58ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_100_0.csv\n",
      "Best checkpoint: checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_70m_training_logs_100_0.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_100_0.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646469-eae3-49d4-8183-14b2ced43f38",
   "metadata": {},
   "source": [
    "# Bias Label (80% accurate, 20% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d7abf6-c327-4b41-9833-f6d8569abbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-70m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 20.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 160/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n",
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 01:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>0.573327</td>\n",
       "      <td>0.669643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.547432</td>\n",
       "      <td>0.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.729700</td>\n",
       "      <td>0.537080</td>\n",
       "      <td>0.758929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.473545</td>\n",
       "      <td>0.863436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.447600</td>\n",
       "      <td>0.447851</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>0.393197</td>\n",
       "      <td>0.859649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.398045</td>\n",
       "      <td>0.847162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>0.331459</td>\n",
       "      <td>0.881579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.790800</td>\n",
       "      <td>0.312818</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.951200</td>\n",
       "      <td>0.270614</td>\n",
       "      <td>0.938326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.254229</td>\n",
       "      <td>0.903930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.224544</td>\n",
       "      <td>0.934211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>0.281872</td>\n",
       "      <td>0.925110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.245063</td>\n",
       "      <td>0.925439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.933921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.564400</td>\n",
       "      <td>0.251270</td>\n",
       "      <td>0.938865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>0.253269</td>\n",
       "      <td>0.938326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.233129</td>\n",
       "      <td>0.925439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>0.267137</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.293258</td>\n",
       "      <td>0.917031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.266645</td>\n",
       "      <td>0.917031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.255823</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.259824</td>\n",
       "      <td>0.908297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.177342</td>\n",
       "      <td>0.947598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.202733</td>\n",
       "      <td>0.930131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.215654</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.210612</td>\n",
       "      <td>0.912664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.183170</td>\n",
       "      <td>0.942731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.187099</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.203067</td>\n",
       "      <td>0.930131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.184280</td>\n",
       "      <td>0.934211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.200704</td>\n",
       "      <td>0.929515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.209867</td>\n",
       "      <td>0.933921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.236596</td>\n",
       "      <td>0.929515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.251511</td>\n",
       "      <td>0.890351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.226692</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.195773</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.201054</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.942731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.942982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.719100</td>\n",
       "      <td>0.206212</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.222506</td>\n",
       "      <td>0.907895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.206954</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.204720</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.193356</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.197033</td>\n",
       "      <td>0.907895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.200522</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.200657</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.191903</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 5 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 4 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 2 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 1 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 2 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 1 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 4 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n",
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 05:44, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "pythia_70m = PythiaModel(\"EleutherAI/pythia-70m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.2)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['ppo_pythia_70m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-70m-deduped-PPO-80-20\"\n",
    "ppo_pythia_70m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['pythia_70m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-70m-reward-model-80-20\"\n",
    "reward_pythia_70m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-70m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_70m.tokenizer.pad_token_id\n",
    "pythia_70m.tokenizer.pad_token = pythia_70m.tokenizer.eos_token\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_70m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-70m-deduped\", \n",
    "    reward_model_config=reward_pythia_70m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_70m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_70m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a5bd25-01ff-46db-9e8b-bb08d0772f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_80_20.csv\n",
      "Best checkpoint: checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_70m_training_logs_80_20.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_80_20.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d45e0-1587-40a8-82bf-3b19f2c7996d",
   "metadata": {},
   "source": [
    "# Bias Label (50% accurate, 50% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedb192d-ec26-47d4-9818-fc52728848e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-70m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 50.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 400/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n",
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-70m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 20789.96 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 3349.41 examples/s]\n",
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 13307.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.869855</td>\n",
       "      <td>0.391892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>0.616456</td>\n",
       "      <td>0.605505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.758800</td>\n",
       "      <td>0.679248</td>\n",
       "      <td>0.559091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.796100</td>\n",
       "      <td>0.656641</td>\n",
       "      <td>0.628319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>1.496687</td>\n",
       "      <td>0.339207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>0.851435</td>\n",
       "      <td>0.434389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>0.696178</td>\n",
       "      <td>0.558559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>0.583769</td>\n",
       "      <td>0.640909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.895242</td>\n",
       "      <td>0.371560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.936557</td>\n",
       "      <td>0.406393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.701800</td>\n",
       "      <td>0.828651</td>\n",
       "      <td>0.416290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.596731</td>\n",
       "      <td>0.631336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.760585</td>\n",
       "      <td>0.398148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>0.734478</td>\n",
       "      <td>0.431925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.861200</td>\n",
       "      <td>0.596190</td>\n",
       "      <td>0.643836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.810700</td>\n",
       "      <td>0.780692</td>\n",
       "      <td>0.434579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.651712</td>\n",
       "      <td>0.594470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>0.840925</td>\n",
       "      <td>0.370892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.716256</td>\n",
       "      <td>0.493213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.652224</td>\n",
       "      <td>0.605505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.746248</td>\n",
       "      <td>0.459821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.727100</td>\n",
       "      <td>0.804134</td>\n",
       "      <td>0.436937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.698512</td>\n",
       "      <td>0.515982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.598400</td>\n",
       "      <td>0.754776</td>\n",
       "      <td>0.476636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.720810</td>\n",
       "      <td>0.522727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>0.811469</td>\n",
       "      <td>0.386667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.711700</td>\n",
       "      <td>0.712739</td>\n",
       "      <td>0.523148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>0.650375</td>\n",
       "      <td>0.622727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.590339</td>\n",
       "      <td>0.675676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>0.621005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.579900</td>\n",
       "      <td>0.644159</td>\n",
       "      <td>0.625592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.701052</td>\n",
       "      <td>0.564220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.697835</td>\n",
       "      <td>0.527027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.734581</td>\n",
       "      <td>0.518349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.859457</td>\n",
       "      <td>0.353211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.736017</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.697932</td>\n",
       "      <td>0.589286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.619404</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.669986</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.699269</td>\n",
       "      <td>0.543379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.687253</td>\n",
       "      <td>0.549107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.616200</td>\n",
       "      <td>0.671756</td>\n",
       "      <td>0.563636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.694889</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.713759</td>\n",
       "      <td>0.538813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.693435</td>\n",
       "      <td>0.562212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.679165</td>\n",
       "      <td>0.561644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.669912</td>\n",
       "      <td>0.591743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.675499</td>\n",
       "      <td>0.605505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 7 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 11 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 9 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 5 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 3 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 2 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 8 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 10 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 12 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 13 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 16 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 15 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 15 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 9 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 4 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 13 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 7 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 10 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 18 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 11 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 5 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/utils.py:815: UserWarning: There are 12 out of 229 instances where the predictions for both options are equal. These instances are ignored in the accuracy computation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                                                                                                                     | 0/801 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 35238.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 05:51, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n",
      "/tmp/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:516: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  rewards[[actual_start, actual_end]] += scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n",
      "Best checkpoint: None\n",
      "Best metric: None\n"
     ]
    }
   ],
   "source": [
    "pythia_70m = PythiaModel(\"EleutherAI/pythia-70m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.5)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['ppo_pythia_70m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-70m-deduped-PPO-50-50\"\n",
    "ppo_pythia_70m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-70m-rlhf-dpo.yaml\")['pythia_70m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-70m-reward-model-50-50\"\n",
    "reward_pythia_70m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-70m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_70m.tokenizer.pad_token_id\n",
    "pythia_70m.tokenizer.pad_token = pythia_70m.tokenizer.eos_token\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_70m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-70m-deduped\", \n",
    "    reward_model_config=reward_pythia_70m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_70m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_70m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()\n",
    "\n",
    "print(f\"Best checkpoint: {ppo_trainer.trainer.state.best_model_checkpoint}\")\n",
    "print(f\"Best metric: {ppo_trainer.trainer.state.best_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ef534c-4019-4016-a1a1-1c978c4e80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_50_50.csv\n",
      "Best checkpoint: checkpoint-30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_70m_training_logs_50_50.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_50_50.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
