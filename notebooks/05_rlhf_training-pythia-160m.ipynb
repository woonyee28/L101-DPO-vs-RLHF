{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89196d6-3730-4270-931e-c61495173b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngnwy289\u001b[0m (\u001b[33mngnwy289-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyn23/l101/notebooks/wandb/run-20251204_185251-yo7ft0f7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/yo7ft0f7' target=\"_blank\">ppo_beta0.1_bias20_run1</a></strong> to <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/yo7ft0f7' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/yo7ft0f7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/yo7ft0f7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x76342934fa10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.models.pythia_model import PythiaModel\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.data.bias_injector import BiasInjector\n",
    "from src.training.rlhf_trainer import RLHF_PPO_Trainer\n",
    "from src.training.utils import load_experiment_config\n",
    "import numpy as np\n",
    "from trl import PPOConfig, RewardConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Code specific to Jupyter Notebook\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = []\n",
    "## Create handler that outputs to notebook\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "## Create formatter\n",
    "formatter = logging.Formatter('%(levelname)s - %(name)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "## Add handler to logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"l101\",           \n",
    "    name=\"ppo_beta0.1_bias20_run1\",     \n",
    "    config={\n",
    "        \"kl_coef\": 0.1,\n",
    "        \"training_data_size\": 801,\n",
    "        \"bias_level\": 0,\n",
    "        \"model\": \"pythia-160m\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b1e4f2-585a-4ee1-a603-261e6b8954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e610c3-4fe1-43bc-b90e-c8b356545210",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bias Label (100% accurate, 0% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228cc37c-7d3d-421f-b0bf-adc66f5eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n"
     ]
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f435c13-6b6d-4480-9610-2c4508bdd15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e230d24d-ff50-4ea3-9b90-a931f8f84363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: True\n",
      "Tokenizer loaded: True\n",
      "Train dataset size: 801\n",
      "Sample from dataset: {'prompt': 'What do you think about the assertion that economic disparities between races are primarily due to individual work ethic and choices?', 'chosen': 'Economic disparities between races are influenced by systemic factors, historical context, and institutional barriers, not just individual work ethic and choices. Addressing these disparities requires a multifaceted approach that considers these broader factors.', 'rejected': \"There's a common belief that these disparities are a result of individual choices and work ethic, suggesting that some racial groups are less inclined to make economically beneficial decisions.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model loaded:\", pythia_160m.model is not None)\n",
    "print(\"Tokenizer loaded:\", pythia_160m.tokenizer is not None)\n",
    "print(\"Train dataset size:\", len(train_ds))\n",
    "print(\"Sample from dataset:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903b2ace-e1b3-493c-868d-0d4a2dd69f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad55299-5034-43d6-9aa1-f0eee7af036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 6047.52 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:00<00:00, 3208.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 02:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.548836</td>\n",
       "      <td>7468.000000</td>\n",
       "      <td>-11.453664</td>\n",
       "      <td>-4.689453</td>\n",
       "      <td>3.921606</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>4.012190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.419196</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>-11.563578</td>\n",
       "      <td>-4.286787</td>\n",
       "      <td>4.397629</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>4.586986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.356481</td>\n",
       "      <td>23396.000000</td>\n",
       "      <td>-13.470905</td>\n",
       "      <td>-4.971930</td>\n",
       "      <td>4.781519</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>5.612915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.294829</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-13.086207</td>\n",
       "      <td>-4.007834</td>\n",
       "      <td>5.678610</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>6.343371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.194041</td>\n",
       "      <td>39131.000000</td>\n",
       "      <td>-13.537716</td>\n",
       "      <td>-3.089119</td>\n",
       "      <td>7.088328</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>7.933183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.134959</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-9.903489</td>\n",
       "      <td>-0.124902</td>\n",
       "      <td>9.205011</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>8.218155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.161434</td>\n",
       "      <td>54080.000000</td>\n",
       "      <td>-10.285830</td>\n",
       "      <td>-1.221878</td>\n",
       "      <td>7.344289</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>7.713741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.396092</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-25.938578</td>\n",
       "      <td>-6.759519</td>\n",
       "      <td>13.200237</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>14.873287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.273453</td>\n",
       "      <td>69715.000000</td>\n",
       "      <td>-18.391703</td>\n",
       "      <td>-1.295704</td>\n",
       "      <td>15.944504</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>14.934639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.257219</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>-29.326509</td>\n",
       "      <td>-8.329236</td>\n",
       "      <td>15.010304</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>19.847858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.193923</td>\n",
       "      <td>85674.000000</td>\n",
       "      <td>-22.195043</td>\n",
       "      <td>-2.839292</td>\n",
       "      <td>19.067349</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>20.167892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.133797</td>\n",
       "      <td>93254.000000</td>\n",
       "      <td>-20.781250</td>\n",
       "      <td>-1.519060</td>\n",
       "      <td>21.109914</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>20.160119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>100737.000000</td>\n",
       "      <td>-27.965517</td>\n",
       "      <td>-5.056266</td>\n",
       "      <td>22.318023</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>23.679898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464801</td>\n",
       "      <td>108276.000000</td>\n",
       "      <td>-31.167026</td>\n",
       "      <td>-6.001084</td>\n",
       "      <td>24.499731</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>25.179322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.395624</td>\n",
       "      <td>116353.000000</td>\n",
       "      <td>-19.835668</td>\n",
       "      <td>2.152396</td>\n",
       "      <td>26.648168</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>23.558277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409468</td>\n",
       "      <td>124268.000000</td>\n",
       "      <td>-17.899246</td>\n",
       "      <td>4.357812</td>\n",
       "      <td>27.942888</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>23.090209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415649</td>\n",
       "      <td>131748.000000</td>\n",
       "      <td>-25.892780</td>\n",
       "      <td>-1.816949</td>\n",
       "      <td>25.204741</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>25.944882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455050</td>\n",
       "      <td>139887.000000</td>\n",
       "      <td>-40.971983</td>\n",
       "      <td>-11.264458</td>\n",
       "      <td>24.735924</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>29.129885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>0.355122</td>\n",
       "      <td>147526.000000</td>\n",
       "      <td>-31.481681</td>\n",
       "      <td>-5.942515</td>\n",
       "      <td>24.667598</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>26.143037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.340440</td>\n",
       "      <td>155236.000000</td>\n",
       "      <td>-28.216595</td>\n",
       "      <td>-2.257021</td>\n",
       "      <td>28.568359</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>27.700818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384438</td>\n",
       "      <td>162598.000000</td>\n",
       "      <td>-24.028017</td>\n",
       "      <td>2.258055</td>\n",
       "      <td>31.737608</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>28.243960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361248</td>\n",
       "      <td>170334.000000</td>\n",
       "      <td>-25.072737</td>\n",
       "      <td>1.885285</td>\n",
       "      <td>32.157328</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>29.183741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.403117</td>\n",
       "      <td>178290.000000</td>\n",
       "      <td>-24.776401</td>\n",
       "      <td>1.786124</td>\n",
       "      <td>31.862608</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>28.811069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397036</td>\n",
       "      <td>185734.000000</td>\n",
       "      <td>-22.455550</td>\n",
       "      <td>4.399532</td>\n",
       "      <td>33.098060</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>29.073032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.079270</td>\n",
       "      <td>193584.000000</td>\n",
       "      <td>-27.127155</td>\n",
       "      <td>-1.367170</td>\n",
       "      <td>27.004041</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>28.617287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>201496.000000</td>\n",
       "      <td>-19.386853</td>\n",
       "      <td>7.220795</td>\n",
       "      <td>32.904095</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>26.788585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.398130</td>\n",
       "      <td>209362.000000</td>\n",
       "      <td>-22.332435</td>\n",
       "      <td>4.416489</td>\n",
       "      <td>28.816810</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>26.331690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405326</td>\n",
       "      <td>216974.000000</td>\n",
       "      <td>-26.949353</td>\n",
       "      <td>1.475185</td>\n",
       "      <td>28.487069</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>28.589429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.379129</td>\n",
       "      <td>224670.000000</td>\n",
       "      <td>-25.019397</td>\n",
       "      <td>2.134234</td>\n",
       "      <td>28.252155</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.159569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336073</td>\n",
       "      <td>232737.000000</td>\n",
       "      <td>-25.945043</td>\n",
       "      <td>0.668410</td>\n",
       "      <td>27.099677</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>27.291865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314552</td>\n",
       "      <td>240514.000000</td>\n",
       "      <td>-25.790948</td>\n",
       "      <td>1.578371</td>\n",
       "      <td>27.959052</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>27.797452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>248165.000000</td>\n",
       "      <td>-23.667834</td>\n",
       "      <td>3.220840</td>\n",
       "      <td>28.227371</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.223406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.211058</td>\n",
       "      <td>256028.000000</td>\n",
       "      <td>-23.992861</td>\n",
       "      <td>2.998307</td>\n",
       "      <td>27.697198</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>26.824962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127475</td>\n",
       "      <td>264174.000000</td>\n",
       "      <td>-25.328394</td>\n",
       "      <td>1.860464</td>\n",
       "      <td>26.705819</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.239152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316552</td>\n",
       "      <td>271805.000000</td>\n",
       "      <td>-24.077990</td>\n",
       "      <td>3.447590</td>\n",
       "      <td>28.164871</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>27.425297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366165</td>\n",
       "      <td>279339.000000</td>\n",
       "      <td>-23.947872</td>\n",
       "      <td>4.021847</td>\n",
       "      <td>29.219828</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.427043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325737</td>\n",
       "      <td>286829.000000</td>\n",
       "      <td>-23.690935</td>\n",
       "      <td>4.053779</td>\n",
       "      <td>29.173491</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>27.903951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365441</td>\n",
       "      <td>294581.000000</td>\n",
       "      <td>-24.391433</td>\n",
       "      <td>3.928371</td>\n",
       "      <td>29.025862</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.747148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344247</td>\n",
       "      <td>302142.000000</td>\n",
       "      <td>-24.212150</td>\n",
       "      <td>4.239195</td>\n",
       "      <td>29.299569</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.827342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391858</td>\n",
       "      <td>309956.000000</td>\n",
       "      <td>-24.629580</td>\n",
       "      <td>4.087693</td>\n",
       "      <td>29.285560</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>27.920537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361996</td>\n",
       "      <td>317428.000000</td>\n",
       "      <td>-24.733028</td>\n",
       "      <td>4.180660</td>\n",
       "      <td>29.636853</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>28.233095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309485</td>\n",
       "      <td>324967.000000</td>\n",
       "      <td>-24.871835</td>\n",
       "      <td>3.799474</td>\n",
       "      <td>28.780172</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>28.364248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320165</td>\n",
       "      <td>333153.000000</td>\n",
       "      <td>-25.159213</td>\n",
       "      <td>3.853032</td>\n",
       "      <td>29.342672</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>28.316499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.295339</td>\n",
       "      <td>340740.000000</td>\n",
       "      <td>-25.612069</td>\n",
       "      <td>3.481793</td>\n",
       "      <td>29.257543</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>28.627951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>348249.000000</td>\n",
       "      <td>-26.820043</td>\n",
       "      <td>2.490915</td>\n",
       "      <td>28.627155</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>28.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225036</td>\n",
       "      <td>356444.000000</td>\n",
       "      <td>-27.076105</td>\n",
       "      <td>2.397012</td>\n",
       "      <td>28.348060</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>29.011965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268676</td>\n",
       "      <td>364472.000000</td>\n",
       "      <td>-26.593876</td>\n",
       "      <td>2.787529</td>\n",
       "      <td>29.092672</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>29.114159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273714</td>\n",
       "      <td>372016.000000</td>\n",
       "      <td>-26.424165</td>\n",
       "      <td>2.972308</td>\n",
       "      <td>29.455819</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>29.336787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290718</td>\n",
       "      <td>379974.000000</td>\n",
       "      <td>-26.682920</td>\n",
       "      <td>2.848939</td>\n",
       "      <td>29.285560</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>29.203110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281702</td>\n",
       "      <td>387547.000000</td>\n",
       "      <td>-26.491918</td>\n",
       "      <td>3.015429</td>\n",
       "      <td>29.390086</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>29.059204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b269d4df-14fd-4a1b-9bdb-686344364f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 10:02, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fdad98-a368-4b0f-b437-5aaded58ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_100_0.csv\n",
      "Best checkpoint: checkpoint-2290\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_100_0.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_100_0.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646469-eae3-49d4-8183-14b2ced43f38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bias Label (80% accurate, 20% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d7abf6-c327-4b41-9833-f6d8569abbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 20.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 160/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 6683.35 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:00<00:00, 2535.95 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 02:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>0.722108</td>\n",
       "      <td>7468.000000</td>\n",
       "      <td>-7.453125</td>\n",
       "      <td>-6.777546</td>\n",
       "      <td>-6.145474</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.025458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.630800</td>\n",
       "      <td>0.468839</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>-10.559267</td>\n",
       "      <td>-8.645003</td>\n",
       "      <td>-6.240841</td>\n",
       "      <td>0.771552</td>\n",
       "      <td>1.115706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>0.849855</td>\n",
       "      <td>23396.000000</td>\n",
       "      <td>-7.471983</td>\n",
       "      <td>-6.214844</td>\n",
       "      <td>-4.914871</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>-0.005927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-16.239224</td>\n",
       "      <td>-11.717268</td>\n",
       "      <td>-5.841595</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>2.752694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.346631</td>\n",
       "      <td>39131.000000</td>\n",
       "      <td>-15.092672</td>\n",
       "      <td>-11.207368</td>\n",
       "      <td>-6.207974</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>2.651536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.149600</td>\n",
       "      <td>0.729709</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-10.623922</td>\n",
       "      <td>-8.613416</td>\n",
       "      <td>-6.681034</td>\n",
       "      <td>0.702586</td>\n",
       "      <td>0.561961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.323441</td>\n",
       "      <td>54080.000000</td>\n",
       "      <td>-10.330819</td>\n",
       "      <td>-8.825768</td>\n",
       "      <td>-7.447737</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>1.180361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.261393</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-9.177263</td>\n",
       "      <td>-7.186591</td>\n",
       "      <td>-5.256196</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>1.675579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.735600</td>\n",
       "      <td>0.277532</td>\n",
       "      <td>69715.000000</td>\n",
       "      <td>-10.237069</td>\n",
       "      <td>-7.970770</td>\n",
       "      <td>-5.583513</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>1.839978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.415019</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>-7.968211</td>\n",
       "      <td>-6.252357</td>\n",
       "      <td>-4.713362</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>1.005523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.286720</td>\n",
       "      <td>85674.000000</td>\n",
       "      <td>-10.452586</td>\n",
       "      <td>-8.598734</td>\n",
       "      <td>-6.753233</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.521013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.258330</td>\n",
       "      <td>93254.000000</td>\n",
       "      <td>-11.928879</td>\n",
       "      <td>-9.462487</td>\n",
       "      <td>-7.092672</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>1.889952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.643500</td>\n",
       "      <td>0.236069</td>\n",
       "      <td>100737.000000</td>\n",
       "      <td>-13.463362</td>\n",
       "      <td>-10.135035</td>\n",
       "      <td>-6.935345</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>2.468885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.608600</td>\n",
       "      <td>0.325354</td>\n",
       "      <td>108276.000000</td>\n",
       "      <td>-10.369612</td>\n",
       "      <td>-8.625808</td>\n",
       "      <td>-7.085668</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>1.314925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.242943</td>\n",
       "      <td>116353.000000</td>\n",
       "      <td>-12.142241</td>\n",
       "      <td>-9.448882</td>\n",
       "      <td>-6.633082</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>2.071794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.270928</td>\n",
       "      <td>124268.000000</td>\n",
       "      <td>-12.244612</td>\n",
       "      <td>-9.392309</td>\n",
       "      <td>-6.316810</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>2.254445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.243809</td>\n",
       "      <td>131748.000000</td>\n",
       "      <td>-13.142241</td>\n",
       "      <td>-9.672144</td>\n",
       "      <td>-5.933728</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>2.695043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.763600</td>\n",
       "      <td>0.203370</td>\n",
       "      <td>139887.000000</td>\n",
       "      <td>-11.508621</td>\n",
       "      <td>-8.703866</td>\n",
       "      <td>-5.673491</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.344227</td>\n",
       "      <td>147526.000000</td>\n",
       "      <td>-9.345366</td>\n",
       "      <td>-6.657092</td>\n",
       "      <td>-3.909752</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>1.808526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.196551</td>\n",
       "      <td>155236.000000</td>\n",
       "      <td>-10.121228</td>\n",
       "      <td>-7.345619</td>\n",
       "      <td>-4.406654</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>2.428374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.282161</td>\n",
       "      <td>162598.000000</td>\n",
       "      <td>-9.235453</td>\n",
       "      <td>-6.427103</td>\n",
       "      <td>-3.475283</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>1.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.211987</td>\n",
       "      <td>170334.000000</td>\n",
       "      <td>-10.551185</td>\n",
       "      <td>-7.836510</td>\n",
       "      <td>-4.967942</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.286032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>0.199065</td>\n",
       "      <td>178290.000000</td>\n",
       "      <td>-11.441810</td>\n",
       "      <td>-8.159651</td>\n",
       "      <td>-4.312500</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>0.253563</td>\n",
       "      <td>185734.000000</td>\n",
       "      <td>-9.990841</td>\n",
       "      <td>-7.592874</td>\n",
       "      <td>-5.158675</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.812365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>0.169298</td>\n",
       "      <td>193584.000000</td>\n",
       "      <td>-11.690733</td>\n",
       "      <td>-8.877121</td>\n",
       "      <td>-5.862338</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>2.456695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.164162</td>\n",
       "      <td>201496.000000</td>\n",
       "      <td>-12.603448</td>\n",
       "      <td>-9.234308</td>\n",
       "      <td>-5.696659</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.847926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.182861</td>\n",
       "      <td>209362.000000</td>\n",
       "      <td>-11.369612</td>\n",
       "      <td>-8.414029</td>\n",
       "      <td>-5.343481</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.426522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.165865</td>\n",
       "      <td>216974.000000</td>\n",
       "      <td>-11.870690</td>\n",
       "      <td>-8.822535</td>\n",
       "      <td>-5.710668</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>2.492861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.181739</td>\n",
       "      <td>224670.000000</td>\n",
       "      <td>-11.250000</td>\n",
       "      <td>-8.164509</td>\n",
       "      <td>-4.892578</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>2.339625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.366749</td>\n",
       "      <td>232737.000000</td>\n",
       "      <td>-9.795797</td>\n",
       "      <td>-6.922309</td>\n",
       "      <td>-3.530543</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>1.696735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.163564</td>\n",
       "      <td>240514.000000</td>\n",
       "      <td>-11.453664</td>\n",
       "      <td>-7.965248</td>\n",
       "      <td>-4.337284</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>2.707974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>0.230837</td>\n",
       "      <td>248165.000000</td>\n",
       "      <td>-10.375000</td>\n",
       "      <td>-7.554014</td>\n",
       "      <td>-4.364224</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>2.069908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.203110</td>\n",
       "      <td>256028.000000</td>\n",
       "      <td>-10.992996</td>\n",
       "      <td>-8.213766</td>\n",
       "      <td>-5.283405</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>2.161638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.236859</td>\n",
       "      <td>264174.000000</td>\n",
       "      <td>-10.365302</td>\n",
       "      <td>-7.827418</td>\n",
       "      <td>-5.168373</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.858634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.212682</td>\n",
       "      <td>271805.000000</td>\n",
       "      <td>-11.112069</td>\n",
       "      <td>-8.309200</td>\n",
       "      <td>-5.390625</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>2.145339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.179305</td>\n",
       "      <td>279339.000000</td>\n",
       "      <td>-12.739763</td>\n",
       "      <td>-9.132408</td>\n",
       "      <td>-5.472252</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>2.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.226865</td>\n",
       "      <td>286829.000000</td>\n",
       "      <td>-10.874461</td>\n",
       "      <td>-7.968295</td>\n",
       "      <td>-5.127290</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>2.117019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.478500</td>\n",
       "      <td>0.208812</td>\n",
       "      <td>294581.000000</td>\n",
       "      <td>-11.688039</td>\n",
       "      <td>-8.420494</td>\n",
       "      <td>-5.181304</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.379647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.181345</td>\n",
       "      <td>302142.000000</td>\n",
       "      <td>-12.335668</td>\n",
       "      <td>-8.647360</td>\n",
       "      <td>-5.100754</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>2.743804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.157183</td>\n",
       "      <td>309956.000000</td>\n",
       "      <td>-12.548491</td>\n",
       "      <td>-8.814184</td>\n",
       "      <td>-5.182651</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>2.917161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.215729</td>\n",
       "      <td>317428.000000</td>\n",
       "      <td>-11.093750</td>\n",
       "      <td>-7.967033</td>\n",
       "      <td>-4.843481</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>2.306775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.285829</td>\n",
       "      <td>324967.000000</td>\n",
       "      <td>-10.310884</td>\n",
       "      <td>-7.179502</td>\n",
       "      <td>-3.967538</td>\n",
       "      <td>0.909483</td>\n",
       "      <td>2.019498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.311044</td>\n",
       "      <td>333153.000000</td>\n",
       "      <td>-10.282866</td>\n",
       "      <td>-7.303559</td>\n",
       "      <td>-4.257408</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>1.876448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.210034</td>\n",
       "      <td>340740.000000</td>\n",
       "      <td>-11.352371</td>\n",
       "      <td>-8.176926</td>\n",
       "      <td>-5.146552</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.352775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.218202</td>\n",
       "      <td>348249.000000</td>\n",
       "      <td>-11.057651</td>\n",
       "      <td>-7.958244</td>\n",
       "      <td>-4.912446</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.264547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.196626</td>\n",
       "      <td>356444.000000</td>\n",
       "      <td>-11.714978</td>\n",
       "      <td>-8.418171</td>\n",
       "      <td>-5.212284</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>2.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.185604</td>\n",
       "      <td>364472.000000</td>\n",
       "      <td>-11.912716</td>\n",
       "      <td>-8.494780</td>\n",
       "      <td>-5.176994</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>2.667497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.198048</td>\n",
       "      <td>372016.000000</td>\n",
       "      <td>-11.735453</td>\n",
       "      <td>-8.460331</td>\n",
       "      <td>-5.277209</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.560210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.182241</td>\n",
       "      <td>379974.000000</td>\n",
       "      <td>-12.039871</td>\n",
       "      <td>-8.575061</td>\n",
       "      <td>-5.216325</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>2.713025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.176736</td>\n",
       "      <td>387547.000000</td>\n",
       "      <td>-11.989224</td>\n",
       "      <td>-8.611126</td>\n",
       "      <td>-5.200970</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>2.766703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 09:36, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.2)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-160m-deduped-PPO-80-20\"\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-160m-reward-model-80-20\"\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a5bd25-01ff-46db-9e8b-bb08d0772f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_80_20.csv\n",
      "Best checkpoint: checkpoint-2310\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_80_20.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_80_20.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d45e0-1587-40a8-82bf-3b19f2c7996d",
   "metadata": {},
   "source": [
    "# Bias Label (50% accurate, 50% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb192d-ec26-47d4-9818-fc52728848e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 50.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 400/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 18199.06 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:00<00:00, 13054.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='505' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [505/505 01:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.754600</td>\n",
       "      <td>0.775056</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>-7.584052</td>\n",
       "      <td>-6.988079</td>\n",
       "      <td>-6.363685</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>-0.108971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.776300</td>\n",
       "      <td>0.861856</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-7.275862</td>\n",
       "      <td>-6.616581</td>\n",
       "      <td>-5.877155</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>-0.231277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.777325</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-7.363147</td>\n",
       "      <td>-6.707839</td>\n",
       "      <td>-5.913793</td>\n",
       "      <td>0.478448</td>\n",
       "      <td>-0.069504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>0.629044</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-8.012931</td>\n",
       "      <td>-7.263605</td>\n",
       "      <td>-6.574353</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.205011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.733600</td>\n",
       "      <td>0.892866</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>-7.177263</td>\n",
       "      <td>-6.290073</td>\n",
       "      <td>-5.349677</td>\n",
       "      <td>0.409483</td>\n",
       "      <td>-0.232893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.773130</td>\n",
       "      <td>92969.000000</td>\n",
       "      <td>-6.901401</td>\n",
       "      <td>-6.262055</td>\n",
       "      <td>-5.550108</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>-0.053206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.735900</td>\n",
       "      <td>0.907912</td>\n",
       "      <td>107874.000000</td>\n",
       "      <td>-6.837823</td>\n",
       "      <td>-5.880321</td>\n",
       "      <td>-4.911099</td>\n",
       "      <td>0.426724</td>\n",
       "      <td>-0.245555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.630951</td>\n",
       "      <td>123773.000000</td>\n",
       "      <td>-7.824353</td>\n",
       "      <td>-6.829809</td>\n",
       "      <td>-5.668103</td>\n",
       "      <td>0.560345</td>\n",
       "      <td>0.280038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.717500</td>\n",
       "      <td>0.797837</td>\n",
       "      <td>139516.000000</td>\n",
       "      <td>-7.132543</td>\n",
       "      <td>-6.491312</td>\n",
       "      <td>-5.860991</td>\n",
       "      <td>0.383621</td>\n",
       "      <td>-0.128637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.708222</td>\n",
       "      <td>154856.000000</td>\n",
       "      <td>-7.547953</td>\n",
       "      <td>-6.991177</td>\n",
       "      <td>-6.412177</td>\n",
       "      <td>0.487069</td>\n",
       "      <td>0.031385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.676674</td>\n",
       "      <td>169636.000000</td>\n",
       "      <td>-7.685345</td>\n",
       "      <td>-7.041554</td>\n",
       "      <td>-6.380388</td>\n",
       "      <td>0.538793</td>\n",
       "      <td>0.115436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>0.891998</td>\n",
       "      <td>184927.000000</td>\n",
       "      <td>-7.307112</td>\n",
       "      <td>-6.314251</td>\n",
       "      <td>-5.415409</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>-0.232220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.887457</td>\n",
       "      <td>200657.000000</td>\n",
       "      <td>-7.401401</td>\n",
       "      <td>-6.555361</td>\n",
       "      <td>-5.719289</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>-0.226024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.665892</td>\n",
       "      <td>216246.000000</td>\n",
       "      <td>-7.794181</td>\n",
       "      <td>-6.972993</td>\n",
       "      <td>-6.210668</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.161773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.750553</td>\n",
       "      <td>231879.000000</td>\n",
       "      <td>-7.279634</td>\n",
       "      <td>-6.602303</td>\n",
       "      <td>-5.916487</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>-0.022764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.632600</td>\n",
       "      <td>0.629119</td>\n",
       "      <td>247082.000000</td>\n",
       "      <td>-8.244073</td>\n",
       "      <td>-7.136719</td>\n",
       "      <td>-6.052802</td>\n",
       "      <td>0.556034</td>\n",
       "      <td>0.283944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.741702</td>\n",
       "      <td>262936.000000</td>\n",
       "      <td>-7.128233</td>\n",
       "      <td>-6.401468</td>\n",
       "      <td>-5.751616</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.003367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.794768</td>\n",
       "      <td>278089.000000</td>\n",
       "      <td>-6.793642</td>\n",
       "      <td>-5.995622</td>\n",
       "      <td>-5.174030</td>\n",
       "      <td>0.452586</td>\n",
       "      <td>-0.082570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.933953</td>\n",
       "      <td>293169.000000</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>-5.567585</td>\n",
       "      <td>-4.518588</td>\n",
       "      <td>0.349138</td>\n",
       "      <td>-0.285897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.633976</td>\n",
       "      <td>308866.000000</td>\n",
       "      <td>-7.967134</td>\n",
       "      <td>-6.735251</td>\n",
       "      <td>-5.526401</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.321525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.962422</td>\n",
       "      <td>323445.000000</td>\n",
       "      <td>-6.747306</td>\n",
       "      <td>-5.720905</td>\n",
       "      <td>-4.591595</td>\n",
       "      <td>0.383621</td>\n",
       "      <td>-0.300916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.542100</td>\n",
       "      <td>0.901982</td>\n",
       "      <td>339303.000000</td>\n",
       "      <td>-6.946659</td>\n",
       "      <td>-6.042935</td>\n",
       "      <td>-4.993265</td>\n",
       "      <td>0.392241</td>\n",
       "      <td>-0.227169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.783173</td>\n",
       "      <td>354811.000000</td>\n",
       "      <td>-7.202586</td>\n",
       "      <td>-6.354122</td>\n",
       "      <td>-5.484375</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>-0.043642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.717811</td>\n",
       "      <td>370600.000000</td>\n",
       "      <td>-7.572737</td>\n",
       "      <td>-6.635035</td>\n",
       "      <td>-5.768858</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.072064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>386144.000000</td>\n",
       "      <td>-7.704203</td>\n",
       "      <td>-6.700700</td>\n",
       "      <td>-5.811422</td>\n",
       "      <td>0.530172</td>\n",
       "      <td>0.121498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/251 04:39 < 04:24, 0.46 it/s, Epoch 2.58/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.5)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-160m-deduped-PPO-50-50\"\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-160m-reward-model-50-50\"\n",
    "reward_args['per_device_train_batch_size'] = 8\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef534c-4019-4016-a1a1-1c978c4e80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_50_50.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_50_50.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511dc4f-e106-4282-a7ae-01324f1364d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
