{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89196d6-3730-4270-931e-c61495173b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngnwy289\u001b[0m (\u001b[33mngnwy289-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyn23/l101/notebooks/wandb/run-20251121_134533-nz6rchsn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/nz6rchsn' target=\"_blank\">ppo_beta0.1_bias20_run1</a></strong> to <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/nz6rchsn' target=\"_blank\">https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/nz6rchsn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ngnwy289-nanyang-technological-university-singapore/l101/runs/nz6rchsn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0446d8c380>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.models.pythia_model import PythiaModel\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.data.bias_injector import BiasInjector\n",
    "from src.training.rlhf_trainer import RLHF_PPO_Trainer\n",
    "from src.training.utils import load_experiment_config\n",
    "import numpy as np\n",
    "from trl import PPOConfig, RewardConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Code specific to Jupyter Notebook\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = []\n",
    "## Create handler that outputs to notebook\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "## Create formatter\n",
    "formatter = logging.Formatter('%(levelname)s - %(name)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "## Add handler to logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"l101\",           \n",
    "    name=\"ppo_beta0.1_bias20_run1\",     \n",
    "    config={\n",
    "        \"kl_coef\": 0.1,\n",
    "        \"training_data_size\": 801,\n",
    "        \"bias_level\": 0,\n",
    "        \"model\": \"pythia-160m\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b1e4f2-585a-4ee1-a603-261e6b8954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e610c3-4fe1-43bc-b90e-c8b356545210",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bias Label (100% accurate, 0% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228cc37c-7d3d-421f-b0bf-adc66f5eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n"
     ]
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f435c13-6b6d-4480-9610-2c4508bdd15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e230d24d-ff50-4ea3-9b90-a931f8f84363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: True\n",
      "Tokenizer loaded: True\n",
      "Train dataset size: 801\n",
      "Sample from dataset: {'prompt': 'What do you think about the assertion that economic disparities between races are primarily due to individual work ethic and choices?', 'chosen': 'Economic disparities between races are influenced by systemic factors, historical context, and institutional barriers, not just individual work ethic and choices. Addressing these disparities requires a multifaceted approach that considers these broader factors.', 'rejected': \"There's a common belief that these disparities are a result of individual choices and work ethic, suggesting that some racial groups are less inclined to make economically beneficial decisions.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model loaded:\", pythia_160m.model is not None)\n",
    "print(\"Tokenizer loaded:\", pythia_160m.tokenizer is not None)\n",
    "print(\"Train dataset size:\", len(train_ds))\n",
    "print(\"Sample from dataset:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903b2ace-e1b3-493c-868d-0d4a2dd69f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad55299-5034-43d6-9aa1-f0eee7af036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|█████████████████████████████████| 801/801 [00:00<00:00, 13230.31 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|██████████████████████████████████| 229/229 [00:00<00:00, 10599.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 02:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.480333</td>\n",
       "      <td>7468.000000</td>\n",
       "      <td>-1.621952</td>\n",
       "      <td>1.586030</td>\n",
       "      <td>5.417969</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>1.847050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.310993</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>0.219929</td>\n",
       "      <td>3.280487</td>\n",
       "      <td>6.292834</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>2.101793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.242637</td>\n",
       "      <td>23396.000000</td>\n",
       "      <td>-2.510319</td>\n",
       "      <td>2.088904</td>\n",
       "      <td>6.417026</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>3.325466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-2.814756</td>\n",
       "      <td>3.882645</td>\n",
       "      <td>9.146013</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>5.305443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.321683</td>\n",
       "      <td>39131.000000</td>\n",
       "      <td>-27.285560</td>\n",
       "      <td>-8.264739</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>13.897339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-25.794181</td>\n",
       "      <td>-6.037261</td>\n",
       "      <td>14.761584</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>16.741539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.289575</td>\n",
       "      <td>54080.000000</td>\n",
       "      <td>-19.637392</td>\n",
       "      <td>-0.806195</td>\n",
       "      <td>18.116918</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>17.680951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.282243</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-16.762662</td>\n",
       "      <td>2.347212</td>\n",
       "      <td>20.858836</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>18.374972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.274037</td>\n",
       "      <td>69715.000000</td>\n",
       "      <td>-14.280846</td>\n",
       "      <td>3.668330</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>17.162404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.341877</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>-18.323815</td>\n",
       "      <td>-0.888119</td>\n",
       "      <td>18.325869</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>16.414582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.297723</td>\n",
       "      <td>85674.000000</td>\n",
       "      <td>-25.978448</td>\n",
       "      <td>-9.022904</td>\n",
       "      <td>11.207301</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>16.022422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.288830</td>\n",
       "      <td>93254.000000</td>\n",
       "      <td>-13.911773</td>\n",
       "      <td>1.110219</td>\n",
       "      <td>17.420797</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>13.515242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.247991</td>\n",
       "      <td>100737.000000</td>\n",
       "      <td>-18.246094</td>\n",
       "      <td>1.714881</td>\n",
       "      <td>21.786638</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>18.868637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241881</td>\n",
       "      <td>108276.000000</td>\n",
       "      <td>-31.136853</td>\n",
       "      <td>-5.007178</td>\n",
       "      <td>20.757004</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>25.185596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.259631</td>\n",
       "      <td>116353.000000</td>\n",
       "      <td>-31.642241</td>\n",
       "      <td>-5.824034</td>\n",
       "      <td>20.580819</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>25.111420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307715</td>\n",
       "      <td>124268.000000</td>\n",
       "      <td>-27.028017</td>\n",
       "      <td>-2.227177</td>\n",
       "      <td>22.531789</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>24.198461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289508</td>\n",
       "      <td>131748.000000</td>\n",
       "      <td>-27.124461</td>\n",
       "      <td>-2.053316</td>\n",
       "      <td>23.300647</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>24.488561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307161</td>\n",
       "      <td>139887.000000</td>\n",
       "      <td>-31.368534</td>\n",
       "      <td>-4.172037</td>\n",
       "      <td>24.707435</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>26.561216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.300803</td>\n",
       "      <td>147526.000000</td>\n",
       "      <td>-24.292026</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>25.060345</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>24.414260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297193</td>\n",
       "      <td>155236.000000</td>\n",
       "      <td>-23.098060</td>\n",
       "      <td>1.483002</td>\n",
       "      <td>25.770474</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>24.555631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.280760</td>\n",
       "      <td>162598.000000</td>\n",
       "      <td>-24.579203</td>\n",
       "      <td>1.631186</td>\n",
       "      <td>26.248922</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>25.313266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>170334.000000</td>\n",
       "      <td>-27.199353</td>\n",
       "      <td>0.294408</td>\n",
       "      <td>26.177802</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>26.851588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.332703</td>\n",
       "      <td>178290.000000</td>\n",
       "      <td>-29.953664</td>\n",
       "      <td>-1.026972</td>\n",
       "      <td>26.850216</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>29.087952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331239</td>\n",
       "      <td>185734.000000</td>\n",
       "      <td>-32.856681</td>\n",
       "      <td>-3.414408</td>\n",
       "      <td>25.130388</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>29.971713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167350</td>\n",
       "      <td>193584.000000</td>\n",
       "      <td>-32.696121</td>\n",
       "      <td>-3.890836</td>\n",
       "      <td>24.444504</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>29.481101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.190341</td>\n",
       "      <td>201496.000000</td>\n",
       "      <td>-34.669720</td>\n",
       "      <td>-5.662437</td>\n",
       "      <td>23.342672</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>29.823093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.228823</td>\n",
       "      <td>209362.000000</td>\n",
       "      <td>-35.072737</td>\n",
       "      <td>-5.839638</td>\n",
       "      <td>23.314116</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>30.060698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239697</td>\n",
       "      <td>216974.000000</td>\n",
       "      <td>-33.954472</td>\n",
       "      <td>-4.522093</td>\n",
       "      <td>24.746228</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>30.155062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.268451</td>\n",
       "      <td>224670.000000</td>\n",
       "      <td>-34.328125</td>\n",
       "      <td>-4.962001</td>\n",
       "      <td>24.739763</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>30.511098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266937</td>\n",
       "      <td>232737.000000</td>\n",
       "      <td>-34.705819</td>\n",
       "      <td>-5.153337</td>\n",
       "      <td>24.524784</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.443835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297031</td>\n",
       "      <td>240514.000000</td>\n",
       "      <td>-34.803341</td>\n",
       "      <td>-5.015184</td>\n",
       "      <td>24.901940</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>30.682977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162575</td>\n",
       "      <td>248165.000000</td>\n",
       "      <td>-35.503233</td>\n",
       "      <td>-6.557704</td>\n",
       "      <td>23.389009</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.440157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.191099</td>\n",
       "      <td>256028.000000</td>\n",
       "      <td>-35.775862</td>\n",
       "      <td>-6.715874</td>\n",
       "      <td>23.465517</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>30.662575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146689</td>\n",
       "      <td>264174.000000</td>\n",
       "      <td>-36.017241</td>\n",
       "      <td>-6.706155</td>\n",
       "      <td>23.129310</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>30.778655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.169193</td>\n",
       "      <td>271805.000000</td>\n",
       "      <td>-35.476293</td>\n",
       "      <td>-6.038458</td>\n",
       "      <td>23.910560</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>30.757168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286523</td>\n",
       "      <td>279339.000000</td>\n",
       "      <td>-34.502155</td>\n",
       "      <td>-5.204151</td>\n",
       "      <td>24.529634</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.640441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318506</td>\n",
       "      <td>286829.000000</td>\n",
       "      <td>-34.522629</td>\n",
       "      <td>-5.154122</td>\n",
       "      <td>24.725216</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.814550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354667</td>\n",
       "      <td>294581.000000</td>\n",
       "      <td>-34.844828</td>\n",
       "      <td>-5.226607</td>\n",
       "      <td>25.048491</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326919</td>\n",
       "      <td>302142.000000</td>\n",
       "      <td>-35.548491</td>\n",
       "      <td>-5.889070</td>\n",
       "      <td>24.393319</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>30.921357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344341</td>\n",
       "      <td>309956.000000</td>\n",
       "      <td>-35.406250</td>\n",
       "      <td>-6.156356</td>\n",
       "      <td>24.541487</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>31.066683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326432</td>\n",
       "      <td>317428.000000</td>\n",
       "      <td>-35.429957</td>\n",
       "      <td>-6.093312</td>\n",
       "      <td>24.445043</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>30.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288216</td>\n",
       "      <td>324967.000000</td>\n",
       "      <td>-35.437500</td>\n",
       "      <td>-5.903473</td>\n",
       "      <td>24.802263</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>31.151242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282989</td>\n",
       "      <td>333153.000000</td>\n",
       "      <td>-34.996767</td>\n",
       "      <td>-5.938558</td>\n",
       "      <td>24.527478</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.597439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.276869</td>\n",
       "      <td>340740.000000</td>\n",
       "      <td>-36.001078</td>\n",
       "      <td>-6.087459</td>\n",
       "      <td>24.267780</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>31.170802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295429</td>\n",
       "      <td>348249.000000</td>\n",
       "      <td>-35.621767</td>\n",
       "      <td>-5.727625</td>\n",
       "      <td>25.073276</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>30.987351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271214</td>\n",
       "      <td>356444.000000</td>\n",
       "      <td>-35.086207</td>\n",
       "      <td>-5.387742</td>\n",
       "      <td>25.143858</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>31.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254117</td>\n",
       "      <td>364472.000000</td>\n",
       "      <td>-35.065733</td>\n",
       "      <td>-5.287786</td>\n",
       "      <td>25.212284</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>30.898006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276335</td>\n",
       "      <td>372016.000000</td>\n",
       "      <td>-35.219828</td>\n",
       "      <td>-5.448717</td>\n",
       "      <td>25.038254</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>30.998041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267383</td>\n",
       "      <td>379974.000000</td>\n",
       "      <td>-35.029095</td>\n",
       "      <td>-5.209806</td>\n",
       "      <td>25.639547</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>31.024858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254005</td>\n",
       "      <td>387547.000000</td>\n",
       "      <td>-35.163793</td>\n",
       "      <td>-5.522848</td>\n",
       "      <td>25.203125</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>31.106527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b269d4df-14fd-4a1b-9bdb-686344364f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 08:54, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fdad98-a368-4b0f-b437-5aaded58ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_100_0.csv\n",
      "Best checkpoint: checkpoint-2220\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_100_0.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_100_0.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646469-eae3-49d4-8183-14b2ced43f38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bias Label (80% accurate, 20% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7abf6-c327-4b41-9833-f6d8569abbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 20.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 160/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|█████████████████████████████████| 801/801 [00:00<00:00, 13107.46 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|██████████████████████████████████| 229/229 [00:00<00:00, 10675.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/1005 02:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.537784</td>\n",
       "      <td>7468.000000</td>\n",
       "      <td>-5.429418</td>\n",
       "      <td>-4.488147</td>\n",
       "      <td>-3.577317</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.425108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.426742</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>-6.674030</td>\n",
       "      <td>-4.993050</td>\n",
       "      <td>-2.953962</td>\n",
       "      <td>0.831897</td>\n",
       "      <td>0.951670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.955900</td>\n",
       "      <td>0.354531</td>\n",
       "      <td>23396.000000</td>\n",
       "      <td>-7.489224</td>\n",
       "      <td>-5.424022</td>\n",
       "      <td>-3.032126</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>1.361648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.317323</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-10.024784</td>\n",
       "      <td>-6.698360</td>\n",
       "      <td>-3.004984</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>2.234207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.274569</td>\n",
       "      <td>39131.000000</td>\n",
       "      <td>-7.821121</td>\n",
       "      <td>-5.307499</td>\n",
       "      <td>-2.628637</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>1.920831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.876200</td>\n",
       "      <td>0.736408</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-3.732759</td>\n",
       "      <td>-1.597885</td>\n",
       "      <td>0.522166</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.618990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.852400</td>\n",
       "      <td>0.360889</td>\n",
       "      <td>54080.000000</td>\n",
       "      <td>-4.646013</td>\n",
       "      <td>-3.143534</td>\n",
       "      <td>-1.242322</td>\n",
       "      <td>0.909483</td>\n",
       "      <td>1.161949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.326150</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-5.517241</td>\n",
       "      <td>-3.536398</td>\n",
       "      <td>-1.565093</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>1.482834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.312753</td>\n",
       "      <td>69715.000000</td>\n",
       "      <td>-7.007543</td>\n",
       "      <td>-4.735373</td>\n",
       "      <td>-1.715153</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>1.955708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.392284</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>-5.661369</td>\n",
       "      <td>-3.179682</td>\n",
       "      <td>-1.378595</td>\n",
       "      <td>0.892241</td>\n",
       "      <td>1.206907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.286883</td>\n",
       "      <td>85674.000000</td>\n",
       "      <td>-5.485722</td>\n",
       "      <td>-2.930860</td>\n",
       "      <td>0.221410</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>2.066982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.246498</td>\n",
       "      <td>93254.000000</td>\n",
       "      <td>-4.974407</td>\n",
       "      <td>-2.445683</td>\n",
       "      <td>0.449829</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>2.128124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.222341</td>\n",
       "      <td>100737.000000</td>\n",
       "      <td>-7.115302</td>\n",
       "      <td>-4.091702</td>\n",
       "      <td>-0.120629</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>2.612174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>108276.000000</td>\n",
       "      <td>-8.296336</td>\n",
       "      <td>-5.256112</td>\n",
       "      <td>-1.271291</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.679587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.246684</td>\n",
       "      <td>116353.000000</td>\n",
       "      <td>-6.781789</td>\n",
       "      <td>-4.621254</td>\n",
       "      <td>-2.266501</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>1.746986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.251628</td>\n",
       "      <td>124268.000000</td>\n",
       "      <td>-7.851293</td>\n",
       "      <td>-5.475397</td>\n",
       "      <td>-3.092235</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>1.934175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>0.265558</td>\n",
       "      <td>131748.000000</td>\n",
       "      <td>-11.143319</td>\n",
       "      <td>-7.447556</td>\n",
       "      <td>-3.034786</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.779592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.862400</td>\n",
       "      <td>0.263014</td>\n",
       "      <td>139887.000000</td>\n",
       "      <td>-7.263470</td>\n",
       "      <td>-5.202350</td>\n",
       "      <td>-2.929688</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>1.606210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>147526.000000</td>\n",
       "      <td>-6.168642</td>\n",
       "      <td>-4.076206</td>\n",
       "      <td>-2.277209</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>1.176522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.250283</td>\n",
       "      <td>155236.000000</td>\n",
       "      <td>-7.549030</td>\n",
       "      <td>-5.381651</td>\n",
       "      <td>-2.935210</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>1.850519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.250158</td>\n",
       "      <td>162598.000000</td>\n",
       "      <td>-6.422414</td>\n",
       "      <td>-4.253180</td>\n",
       "      <td>-2.092790</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>1.749971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.213436</td>\n",
       "      <td>170334.000000</td>\n",
       "      <td>-6.826509</td>\n",
       "      <td>-4.264314</td>\n",
       "      <td>-1.675596</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>2.224336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.209221</td>\n",
       "      <td>178290.000000</td>\n",
       "      <td>-7.254849</td>\n",
       "      <td>-4.403442</td>\n",
       "      <td>-1.469319</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.236422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.291293</td>\n",
       "      <td>185734.000000</td>\n",
       "      <td>-7.213901</td>\n",
       "      <td>-4.012515</td>\n",
       "      <td>-0.849113</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>1.915443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.172591</td>\n",
       "      <td>193584.000000</td>\n",
       "      <td>-8.391703</td>\n",
       "      <td>-4.833825</td>\n",
       "      <td>-1.235528</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>2.653538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.166834</td>\n",
       "      <td>201496.000000</td>\n",
       "      <td>-9.200431</td>\n",
       "      <td>-5.597672</td>\n",
       "      <td>-1.888093</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>2.792128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.176266</td>\n",
       "      <td>209362.000000</td>\n",
       "      <td>-8.929957</td>\n",
       "      <td>-5.374586</td>\n",
       "      <td>-1.762432</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>2.658828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.156232</td>\n",
       "      <td>216974.000000</td>\n",
       "      <td>-8.852909</td>\n",
       "      <td>-4.983556</td>\n",
       "      <td>-1.088564</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>2.918630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>224670.000000</td>\n",
       "      <td>-8.297414</td>\n",
       "      <td>-4.348727</td>\n",
       "      <td>-0.336961</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>2.944838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.594700</td>\n",
       "      <td>0.256587</td>\n",
       "      <td>232737.000000</td>\n",
       "      <td>-7.209860</td>\n",
       "      <td>-3.369870</td>\n",
       "      <td>0.254445</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>2.184938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.158660</td>\n",
       "      <td>240514.000000</td>\n",
       "      <td>-7.787177</td>\n",
       "      <td>-3.989493</td>\n",
       "      <td>-0.340909</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.777447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.830400</td>\n",
       "      <td>0.151461</td>\n",
       "      <td>248165.000000</td>\n",
       "      <td>-7.225216</td>\n",
       "      <td>-3.336684</td>\n",
       "      <td>0.306270</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>2.770497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>256028.000000</td>\n",
       "      <td>-6.333782</td>\n",
       "      <td>-2.644970</td>\n",
       "      <td>0.834313</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>2.625083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.492200</td>\n",
       "      <td>0.207874</td>\n",
       "      <td>264174.000000</td>\n",
       "      <td>-6.237877</td>\n",
       "      <td>-2.392582</td>\n",
       "      <td>1.171076</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>2.406251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.149382</td>\n",
       "      <td>271805.000000</td>\n",
       "      <td>-7.315194</td>\n",
       "      <td>-3.183075</td>\n",
       "      <td>0.777125</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>3.065153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.148962</td>\n",
       "      <td>279339.000000</td>\n",
       "      <td>-7.613955</td>\n",
       "      <td>-3.306470</td>\n",
       "      <td>1.082873</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>3.319686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.151882</td>\n",
       "      <td>286829.000000</td>\n",
       "      <td>-7.842134</td>\n",
       "      <td>-3.413543</td>\n",
       "      <td>0.963556</td>\n",
       "      <td>0.943966</td>\n",
       "      <td>3.375477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.139738</td>\n",
       "      <td>294581.000000</td>\n",
       "      <td>-7.929957</td>\n",
       "      <td>-3.313363</td>\n",
       "      <td>1.267333</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>3.549615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.335500</td>\n",
       "      <td>0.151152</td>\n",
       "      <td>302142.000000</td>\n",
       "      <td>-7.112338</td>\n",
       "      <td>-2.873590</td>\n",
       "      <td>1.393066</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>3.123207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.106506</td>\n",
       "      <td>309956.000000</td>\n",
       "      <td>-7.518319</td>\n",
       "      <td>-3.227899</td>\n",
       "      <td>1.227777</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>3.611248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.139684</td>\n",
       "      <td>317428.000000</td>\n",
       "      <td>-7.182651</td>\n",
       "      <td>-2.870175</td>\n",
       "      <td>1.438914</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>3.286026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.135286</td>\n",
       "      <td>324967.000000</td>\n",
       "      <td>-7.224138</td>\n",
       "      <td>-2.691940</td>\n",
       "      <td>1.756285</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>3.390745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.198968</td>\n",
       "      <td>333153.000000</td>\n",
       "      <td>-6.983028</td>\n",
       "      <td>-2.531968</td>\n",
       "      <td>1.792733</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>2.973973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.169364</td>\n",
       "      <td>340740.000000</td>\n",
       "      <td>-6.984644</td>\n",
       "      <td>-2.657557</td>\n",
       "      <td>1.606572</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>3.081179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>0.129505</td>\n",
       "      <td>348249.000000</td>\n",
       "      <td>-7.457166</td>\n",
       "      <td>-3.067639</td>\n",
       "      <td>1.336022</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>3.425185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.122284</td>\n",
       "      <td>356444.000000</td>\n",
       "      <td>-7.447468</td>\n",
       "      <td>-2.966808</td>\n",
       "      <td>1.598332</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>3.604902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>364472.000000</td>\n",
       "      <td>-7.394935</td>\n",
       "      <td>-2.868890</td>\n",
       "      <td>1.670410</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>3.532445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.125112</td>\n",
       "      <td>372016.000000</td>\n",
       "      <td>-7.316541</td>\n",
       "      <td>-2.865013</td>\n",
       "      <td>1.680548</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>3.531868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.125063</td>\n",
       "      <td>379974.000000</td>\n",
       "      <td>-7.293642</td>\n",
       "      <td>-2.822238</td>\n",
       "      <td>1.849146</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>3.646006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.129644</td>\n",
       "      <td>387547.000000</td>\n",
       "      <td>-7.276401</td>\n",
       "      <td>-2.807006</td>\n",
       "      <td>1.850173</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>3.622147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 08:45, Epoch 5.01/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.2)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-160m-deduped-PPO-80-20\"\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-160m-reward-model-80-20\"\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5bd25-01ff-46db-9e8b-bb08d0772f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_80_20.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_80_20.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d45e0-1587-40a8-82bf-3b19f2c7996d",
   "metadata": {},
   "source": [
    "# Bias Label (50% accurate, 50% Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedb192d-ec26-47d4-9818-fc52728848e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.models.pythia_model - Loading model EleutherAI/pythia-160m-deduped (revision: step143000) at cache_dir: ./cache_dir\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.dataset_loader - Loading BiasDPO dataset\n",
      "INFO - src.data.dataset_loader - Total samples: 1145\n",
      "INFO - src.data.dataset_loader - Train samples: 801\n",
      "INFO - src.data.dataset_loader - Validation samples: 229\n",
      "INFO - src.data.dataset_loader - Test samples: 115\n",
      "INFO - src.data.bias_injector - Injecting 50.0% bias:\n",
      "INFO - src.data.bias_injector -   - Train: flipping 400/801 examples\n",
      "INFO - src.data.bias_injector - Bias injection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Creating reward model from base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m-deduped and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Filtering train >1024 tokens: 100%|█████████████████████████████████| 801/801 [00:00<00:00, 20922.28 examples/s]\n",
      "Filtering eval >1024 tokens: 100%|██████████████████████████████████| 229/229 [00:00<00:00, 15832.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Training reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='505' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [505/505 01:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>0.613687</td>\n",
       "      <td>15118.000000</td>\n",
       "      <td>-2.299569</td>\n",
       "      <td>-0.981261</td>\n",
       "      <td>0.545468</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.394507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.748200</td>\n",
       "      <td>0.662421</td>\n",
       "      <td>31512.000000</td>\n",
       "      <td>-1.502896</td>\n",
       "      <td>-0.671434</td>\n",
       "      <td>0.228521</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.125950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.749300</td>\n",
       "      <td>0.712304</td>\n",
       "      <td>46204.000000</td>\n",
       "      <td>-0.716477</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>0.240619</td>\n",
       "      <td>0.461207</td>\n",
       "      <td>-0.010828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.647516</td>\n",
       "      <td>61840.000000</td>\n",
       "      <td>-1.646686</td>\n",
       "      <td>-0.764335</td>\n",
       "      <td>0.087003</td>\n",
       "      <td>0.564655</td>\n",
       "      <td>0.162885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.711700</td>\n",
       "      <td>0.920127</td>\n",
       "      <td>77707.000000</td>\n",
       "      <td>0.134036</td>\n",
       "      <td>0.923775</td>\n",
       "      <td>1.672279</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>-0.316372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>92969.000000</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>0.936334</td>\n",
       "      <td>1.768050</td>\n",
       "      <td>0.400862</td>\n",
       "      <td>-0.294161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.626594</td>\n",
       "      <td>107874.000000</td>\n",
       "      <td>-0.828596</td>\n",
       "      <td>-0.136228</td>\n",
       "      <td>0.515678</td>\n",
       "      <td>0.599138</td>\n",
       "      <td>0.186133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.651206</td>\n",
       "      <td>123773.000000</td>\n",
       "      <td>-0.769405</td>\n",
       "      <td>-0.153805</td>\n",
       "      <td>0.422199</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.120937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.731423</td>\n",
       "      <td>139516.000000</td>\n",
       "      <td>-0.449532</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.473145</td>\n",
       "      <td>0.422414</td>\n",
       "      <td>-0.045346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.788807</td>\n",
       "      <td>154856.000000</td>\n",
       "      <td>-0.271076</td>\n",
       "      <td>0.267751</td>\n",
       "      <td>0.869242</td>\n",
       "      <td>0.400862</td>\n",
       "      <td>-0.128403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.630800</td>\n",
       "      <td>0.640511</td>\n",
       "      <td>169636.000000</td>\n",
       "      <td>-1.063477</td>\n",
       "      <td>-0.360771</td>\n",
       "      <td>0.354768</td>\n",
       "      <td>0.590517</td>\n",
       "      <td>0.175251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.675800</td>\n",
       "      <td>0.961891</td>\n",
       "      <td>184927.000000</td>\n",
       "      <td>-0.241876</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>1.824219</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>-0.340458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.746357</td>\n",
       "      <td>200657.000000</td>\n",
       "      <td>-0.557953</td>\n",
       "      <td>0.083692</td>\n",
       "      <td>0.720463</td>\n",
       "      <td>0.478448</td>\n",
       "      <td>-0.048998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.655049</td>\n",
       "      <td>216246.000000</td>\n",
       "      <td>-1.166218</td>\n",
       "      <td>-0.355880</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.590517</td>\n",
       "      <td>0.145846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.717024</td>\n",
       "      <td>231879.000000</td>\n",
       "      <td>-0.484998</td>\n",
       "      <td>0.068102</td>\n",
       "      <td>0.638756</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>0.687389</td>\n",
       "      <td>247082.000000</td>\n",
       "      <td>-0.767553</td>\n",
       "      <td>-0.096812</td>\n",
       "      <td>0.552953</td>\n",
       "      <td>0.564655</td>\n",
       "      <td>0.069713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.587600</td>\n",
       "      <td>0.693515</td>\n",
       "      <td>262936.000000</td>\n",
       "      <td>-0.829977</td>\n",
       "      <td>-0.167801</td>\n",
       "      <td>0.475026</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.060441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>278089.000000</td>\n",
       "      <td>-0.569058</td>\n",
       "      <td>0.052370</td>\n",
       "      <td>0.712710</td>\n",
       "      <td>0.392241</td>\n",
       "      <td>-0.068323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.741962</td>\n",
       "      <td>293169.000000</td>\n",
       "      <td>-0.508896</td>\n",
       "      <td>0.108292</td>\n",
       "      <td>0.759564</td>\n",
       "      <td>0.495690</td>\n",
       "      <td>-0.031049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.668716</td>\n",
       "      <td>308866.000000</td>\n",
       "      <td>-1.094626</td>\n",
       "      <td>-0.277659</td>\n",
       "      <td>0.508470</td>\n",
       "      <td>0.573276</td>\n",
       "      <td>0.129915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>323445.000000</td>\n",
       "      <td>-0.245331</td>\n",
       "      <td>0.616613</td>\n",
       "      <td>1.522309</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>-0.286173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.604300</td>\n",
       "      <td>0.786088</td>\n",
       "      <td>339303.000000</td>\n",
       "      <td>-0.391221</td>\n",
       "      <td>0.289045</td>\n",
       "      <td>1.066019</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>-0.087817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.731688</td>\n",
       "      <td>354811.000000</td>\n",
       "      <td>-0.638806</td>\n",
       "      <td>0.046186</td>\n",
       "      <td>0.742987</td>\n",
       "      <td>0.478448</td>\n",
       "      <td>0.008165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>0.692306</td>\n",
       "      <td>370600.000000</td>\n",
       "      <td>-0.946159</td>\n",
       "      <td>-0.180096</td>\n",
       "      <td>0.558428</td>\n",
       "      <td>0.556034</td>\n",
       "      <td>0.097869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.680344</td>\n",
       "      <td>386144.000000</td>\n",
       "      <td>-1.243484</td>\n",
       "      <td>-0.316963</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.128492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - Reward model training complete!\n",
      "INFO - src.training.rlhf_trainer - Reward model type: <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>\n",
      "INFO - src.training.rlhf_trainer - Reward model has 'score' attribute: True\n",
      "INFO - src.training.rlhf_trainer - Initializing PPOTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyn23/l101/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPOTrainer initialized successfully!\n",
      "INFO - src.training.rlhf_trainer - Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [251/251 08:52, Epoch 5/5.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - src.training.rlhf_trainer - PPO training complete.\n"
     ]
    }
   ],
   "source": [
    "pythia_160m = PythiaModel(\"EleutherAI/pythia-160m-deduped\", \"step143000\", \"./cache_dir\")\n",
    "\n",
    "loader = DatasetLoader()\n",
    "\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "train_ds, valid_ds, test_ds = loader.load_biasDPO()\n",
    "bias_injector = BiasInjector(loader, seed = 42)\n",
    "bias_train_ds, bias_valid_ds, test_ds = bias_injector.inject_bias(bias_ratio = 0.5)\n",
    "\n",
    "train_ds = bias_train_ds\n",
    "valid_ds = bias_valid_ds\n",
    "\n",
    "ppo_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['ppo_pythia_160m_config']\n",
    "ppo_args['output_dir'] = \"./pythia-160m-deduped-PPO-50-50\"\n",
    "ppo_pythia_160m_config = PPOConfig(**ppo_args)\n",
    "\n",
    "\n",
    "reward_args= load_experiment_config(\"../configs/pythia-160m-rlhf-dpo.yaml\")['pythia_160m_reward_config']\n",
    "reward_args['output_dir'] = \"./pythia-160m-reward-model-50-50\"\n",
    "reward_args['per_device_train_batch_size'] = 8\n",
    "reward_pythia_160m_config = RewardConfig(**reward_args)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"EleutherAI/pythia-160m-deduped\",\n",
    "                num_labels=1,\n",
    "                )\n",
    "value_model.config.pad_token_id = pythia_160m.tokenizer.pad_token_id\n",
    "\n",
    "ppo_trainer = RLHF_PPO_Trainer(\n",
    "    model=pythia_160m.model, \n",
    "    reward_model_base=\"EleutherAI/pythia-160m-deduped\", \n",
    "    reward_model_config=reward_pythia_160m_config,\n",
    "    value_model=value_model, \n",
    "    processing_class=pythia_160m.tokenizer, \n",
    "    train_dataset=train_ds, \n",
    "    valid_ds=valid_ds, \n",
    "    args=ppo_pythia_160m_config\n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ef534c-4019-4016-a1a1-1c978c4e80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs saved to ppo_training_logs_50_50.csv\n",
      "Best checkpoint: checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = ppo_trainer.trainer.state\n",
    "logs = state.log_history\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df_every_10 = df[df['step'] % 10 == 0] if 'step' in df.columns else df.iloc[::10]\n",
    "relevant_cols = [col for col in df_every_10.columns if not col.startswith('_')]\n",
    "\n",
    "# print(df_every_10[relevant_cols].to_string(index=False))\n",
    "\n",
    "df_every_10[relevant_cols].to_csv('ppo_160m_training_logs_50_50.csv', index=False)\n",
    "print(\"\\nLogs saved to ppo_training_logs_50_50.csv\")\n",
    "\n",
    "best_checkpoint = f\"checkpoint-{int(df['objective/rlhf_reward'].idxmax()) * 10}\"\n",
    "print(f\"Best checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511dc4f-e106-4282-a7ae-01324f1364d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
